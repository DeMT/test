{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Requester(object):   \n",
    "# base class  \n",
    "    headers = {\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.84 Safari/537.36',\n",
    "\n",
    "    }\n",
    "    \n",
    "    def req(self,url):\n",
    "        connection = True\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        import time\n",
    "        soup=''\n",
    "        rs = requests.session()\n",
    "        while connection == True:           \n",
    "            try:                \n",
    "                res = rs.get(url,headers=self.headers)                \n",
    "                soup = BeautifulSoup(res.text)                \n",
    "                connection = False\n",
    "                \n",
    "                return soup\n",
    "            except:\n",
    "                print ('connection error , sleep 1 min' )                         \n",
    "                time.sleep(60)\n",
    "class Crawler(object):\n",
    "    currentCrawlingTag = ''\n",
    "    domain = 'http://www.wakema.com.tw/'\n",
    "    rs = Requester()    \n",
    "    def tagCrawl(self,url,wantedTag):\n",
    "        soup = self.rs.req(url)\n",
    "        linkList = []\n",
    "        for link in soup.select(wantedTag):\n",
    "            linkList.append(self.domain+link['href'])\n",
    "        return linkList\n",
    "    def storeUrlCrawl(self,url):\n",
    "        import re\n",
    "        result = []\n",
    "        currentTag = ''\n",
    "        hasNext = True         \n",
    "        # example url = http://www.wakema.com.tw/category/CategoryBottom.jsp?t158_id=68&&pageNo=1\n",
    "        while hasNext == True:\n",
    "            try:\n",
    "                soup = self.rs.req(url)\n",
    "                if currentTag == '':\n",
    "                    currentTag=soup.select('.breadcrumb li')[-2].a.text+'@%&'+soup.select('.breadcrumb li')[-1].a.text\n",
    "                stores=soup.select('.col-lg-10.col-md-9.col-xs-12 h4 a')\n",
    "\n",
    "                for store in stores :\n",
    "                    result.append(self.domain+store['href'])\n",
    "                url = self.domain+soup.find_all(alt=re.compile('下一頁'))[0].parent['href']                \n",
    "            except:\n",
    "                print('no next page ,{} done .'.format(currentTag))\n",
    "                self.currentCrawlingTag = currentTag.strip('\\ufeff')\n",
    "                hasNext = False\n",
    "        return result\n",
    "    def contentCrawl(self,urlList):\n",
    "        import re\n",
    "        result = []        \n",
    "        for url in urlList:\n",
    "            soup = self.rs.req(url)\n",
    "            #電話\n",
    "            try:\n",
    "                tel=soup.find_all(href=re.compile(\"tel:\"))[0]['href'].split(':')[1]\n",
    "            except IndexError:\n",
    "                print(url,'cant find tel .')\n",
    "            #所有我能找的到的tag    note: tag應該和其他資訊使用不同的分隔，這個網站tag數量會變化\n",
    "            tagObjects = soup.select('.table tr')[3].select('a')\n",
    "            tag = ''\n",
    "            for tagObject in tagObjects:                \n",
    "                tag +=tagObject.text+'@%&'\n",
    "            tag = tag+self.currentCrawlingTag            \n",
    "            #地址\n",
    "            address = soup.find_all(itemprop=re.compile('address'))[0].text\n",
    "            #店名\n",
    "            store_name = soup.find_all(itemprop=re.compile('name'))[0].text\n",
    "            #官網 沒有官網的紀錄為None\n",
    "            try:\n",
    "                officialSite = soup.select('.text-primary a')[0]\n",
    "                officialSite_Url = officialSite['href']                \n",
    "            except IndexError :\n",
    "                officialSite_Url = 'None'\n",
    "            #商店介紹\n",
    "            store_intro = soup.select('.alicef2')[0].text\n",
    "            #服務項目\n",
    "            service_intro = soup.select('.panel-body')[1].p.text\n",
    "            \n",
    "            result.append(tag+'_|'+store_name+'_|'+tel+'_|'+address+'_|'+store_intro+'_|'+service_intro+'_|'+officialSite_Url+'\\n')\n",
    "            \n",
    "        return result\n",
    "def exitfunc():\n",
    "    print('program finish , clean the doneList. ')\n",
    "    import os\n",
    "    os.remove('doneList.txt')          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michaeltsai\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no next page ,餐飲設備裝置@%&餐飲設備 done .\n",
      "['\\ufeff餐飲設備裝置@%&餐飲設備', '餐飲設備裝置@%&電解水機', '餐飲設備裝置@%&製冰機', '餐飲設備裝置@%&淨水器'] 餐飲設備裝置@%&餐飲設備\n",
      "no next page ,餐飲設備裝置@%&電解水機 done .\n",
      "['\\ufeff餐飲設備裝置@%&餐飲設備', '餐飲設備裝置@%&電解水機', '餐飲設備裝置@%&製冰機', '餐飲設備裝置@%&淨水器'] 餐飲設備裝置@%&電解水機\n",
      "餐飲設備裝置@%&電解水機 crawled berfore , skip it \n",
      "no next page ,餐飲設備裝置@%&製冰機 done .\n",
      "['\\ufeff餐飲設備裝置@%&餐飲設備', '餐飲設備裝置@%&電解水機', '餐飲設備裝置@%&製冰機', '餐飲設備裝置@%&淨水器'] 餐飲設備裝置@%&製冰機\n",
      "餐飲設備裝置@%&製冰機 crawled berfore , skip it \n",
      "connection error , sleep 1 min\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "urlList = []\n",
    "linkList = []\n",
    "tc = Crawler()\n",
    "try:\n",
    "    g = open('wakemaDoneList.txt','r' , encoding='utf-8')\n",
    "except:\n",
    "    with open('wakemaDoneList.txt','a+' , encoding='utf-8') as justCreate:\n",
    "        justCreate.close()\n",
    "    g = open('wakemaDoneList.txt','r' , encoding='utf-8')\n",
    "# 註冊回調函數，如果這隻程式做成.py檔，應該會在執行完畢的時候刪除doneList... 應該啦。\n",
    "sys.exitfunc=exitfunc\n",
    "listBeforeStrip=g.read().splitlines()\n",
    "doneList=list(map(str.strip, listBeforeStrip))\n",
    "g.close()\n",
    "topicList = tc.tagCrawl('http://www.wakema.com.tw/','.gold_mu a')\n",
    "for categoryLink  in topicList :    \n",
    "    categoryList = tc.tagCrawl(categoryLink,'.col-md-6.col-xs-12 a')\n",
    "    for actegoryLink in categoryList:        \n",
    "        urlList=tc.storeUrlCrawl(actegoryLink)\n",
    "        print(doneList , tc.currentCrawlingTag)\n",
    "        if tc.currentCrawlingTag in doneList:\n",
    "            print(tc.currentCrawlingTag,'crawled berfore , skip it ')\n",
    "        result = tc.contentCrawl(urlList)\n",
    "        with open('wakema.txt','a' ,encoding='utf-8') as wakema:\n",
    "            for line in result:\n",
    "                wakema.write(line)\n",
    "        wakemaDoneList = open('wakemaDoneList.txt','a',encoding='utf-8')\n",
    "        wakemaDoneList.write(tc.currentCrawlingTag+'\\n')\n",
    "        wakemaDoneList.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿\n"
     ]
    }
   ],
   "source": [
    "print('\\ufeff')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
